# Data Pipeline Testing & Validation Framework

A production-ready ETL pipeline with comprehensive data quality validation checks. This framework prevents silent data corruption by implementing automated validation at each stage of the data pipeline.

## ğŸ¯ Problem Statement

Data pipelines often fail silently - bad data reaches downstream systems without detection, leading to:
- Incorrect business decisions based on flawed analytics
- Data quality issues discovered too late in production
- Time-consuming manual data quality checks
- Lack of trust in data across teams

This framework solves these problems by implementing automated validation checks that catch data quality issues before they propagate.

## ğŸ—ï¸ Architecture
```
Raw Data (CSV) 
    â†“
[EXTRACT] - Read and validate file existence
    â†“
[TRANSFORM] - Clean, type convert, calculate
    â†“
[VALIDATE] - 8 automated quality checks
    â†“
[LOAD] - Save to database (only if validation passes)
    â†“
Clean Data (SQLite)
```

## âœ¨ Features

### Data Validation Checks
1. **Schema Validation** - Ensures all expected columns are present
2. **Row Count Check** - Verifies no data loss during transformation
3. **Null Value Check** - Detects missing values in critical columns
4. **Data Type Check** - Confirms correct data types for each column
5. **Duplicate Check** - Identifies duplicate records by key column
6. **Range Validation** - Checks values fall within expected ranges
7. **Transformation Accuracy** - Verifies calculated fields are correct
8. **Source vs Target Comparison** - Ensures data integrity across pipeline

### Additional Features
- Comprehensive error handling and logging
- Detailed validation reports
- Database verification after load
- Modular, reusable code structure
- Easy to extend with new validation rules

## ğŸ› ï¸ Tech Stack

- **Python 3.11** - Core programming language
- **Pandas** - Data manipulation and transformation
- **SQLite** - Lightweight database for data storage
- **Git** - Version control

## ğŸ“ Project Structure
```
data-validation-project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Source CSV files
â”‚   â”œâ”€â”€ processed/        # Intermediate cleaned data
â”‚   â””â”€â”€ warehouse/        # Final SQLite database
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py        # Data extraction logic
â”‚   â”œâ”€â”€ transform.py      # Data transformation logic
â”‚   â”œâ”€â”€ load.py           # Database loading logic
â”‚   â”œâ”€â”€ validate.py       # Validation framework
â”‚   â””â”€â”€ pipeline.py       # Master orchestration script
â”œâ”€â”€ tests/                # Unit tests (future enhancement)
â”œâ”€â”€ reports/              # Validation reports (future enhancement)
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### Prerequisites
```bash
python >= 3.8
pip
```

### Installation
```bash
# Clone the repository
git clone https://github.com/rohithdataengineer1/data-pipeline-validation-framework.git
cd data-pipeline-validation-framework

# Install dependencies
pip install -r requirements.txt
```

### Run the Pipeline
```bash
# Run the complete ETL pipeline with validation
python src/pipeline.py
```

### Run Individual Components
```bash
# Extract only
python src/extract.py

# Transform only
python src/transform.py

# Validate only
python src/validate.py

# Load only
python src/load.py
```

## ğŸ“Š Sample Output
```
============================================================
 DATA PIPELINE WITH VALIDATION FRAMEWORK
============================================================

[STEP 1] EXTRACT
âœ“ Extracted 10 rows

[STEP 2] TRANSFORM
âœ“ Transformation complete! 10 rows ready

[STEP 3] VALIDATE
âœ“ Schema Validation: PASSED
âœ“ Row Count Check: PASSED  
âœ“ Null Check: PASSED
âœ“ Data Type Check: PASSED
âœ“ Duplicate Check: PASSED
âœ“ Range Check (price): PASSED
âœ“ Range Check (quantity): PASSED
âœ“ Transformation Accuracy: PASSED

Total Checks: 8
Passed: 8
Failed: 0

ğŸ‰ ALL VALIDATIONS PASSED! ğŸ‰

[STEP 4] LOAD
âœ“ Loaded 10 rows to database

============================================================
âœ“ PIPELINE COMPLETED SUCCESSFULLY!
============================================================
```

## ğŸ“ Key Learnings

- Implementing production-grade data validation patterns
- Building modular, maintainable ETL pipelines
- Error handling and defensive programming in data engineering
- SQL database operations and verification
- Data quality best practices

## ğŸ”® Future Enhancements

- [ ] Add pytest unit tests for all components
- [ ] Generate HTML validation reports
- [ ] Implement email alerts for validation failures
- [ ] Add support for multiple data sources (APIs, databases)
- [ ] Integrate with Apache Airflow for scheduling
- [ ] Add Great Expectations for advanced validation
- [ ] Implement data lineage tracking
- [ ] Add performance metrics and monitoring

## ğŸ“ License

This project is open source and available under the MIT License.

## ğŸ¤ Connect

- **LinkedIn:** [Rohith Vyda](https://www.linkedin.com/in/rohith-vyda)
- **GitHub:** [@rohithdataengineer1](https://github.com/rohithdataengineer1)

---

*Built with focus on data quality and reliability*